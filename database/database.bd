{"title":"HOW POWERFUL ARE GRAPH NEURAL NETWORKS?","author":null,"abstract":"Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and trans- forming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler- Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.","publisher":null,"url":null,"path":"/Users/wubolun/Bowen/SJTU/Netsec&TS Lab/papers/gnn/ICLR_GIN.pdf","tags":["MCBG","AI","GNN"],"remark":"GIN<br>","_id":"FhXbP9BUmctbFgQR"}
{"title":"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks","author":null,"abstract":"BERT (Devlin et al., 2018) and RoBERTa (Liu et  al.,  2019)  has  set  a  new  state-of-the-art performance on sentence-pair regression tasks like  semantic  textual  similarity  (STS).  How- ever,  it  requires  that  both  sentences  are  fed into the network, which causes a massive com- putational  overhead:   Finding  the  most  sim- ilar  pair  in  a  collection  of  10,000  sentences requires about 50 million inference computa- tions (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic sim- ilarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT),  a  modification  of  the  pretrained BERT network that use siamese and triplet net- work structures to derive semantically mean- ingful sentence embeddings that can be com- pared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 sec- onds with SBERT, while maintaining the ac- curacy from BERT. We evaluate SBERT and SRoBERTa on com- mon  STS  tasks  and  transfer  learning  tasks, where   it   outperforms   other   state-of-the-art sentence embeddings methods.","publisher":null,"url":null,"path":"/Users/wubolun/Bowen/SJTU/Netsec&TS Lab/papers/nlp/SBERT.pdf","tags":[],"remark":null,"_id":"i95B2yMeaZg7rLPz"}
{"title":"UNICORN: Runtime Provenance-Based Detector for Advanced Persistent Threats<br>","author":null,"abstract":"Abstract—Advanced Persistent Threats (APTs) are difficult to detect due to their “low-and-slow” attack patterns and frequent use of zero-day exploits. We present UNICORN, an anomaly- based APT detector that effectively leverages data provenance analysis. From modeling to detection, UNICORN tailors its design specifically for the unique characteristics of APTs. Through extensive yet time-efficient graph analysis, UNICORN explores provenance graphs that provide rich contextual and historical information to identify stealthy anomalous activities without pre- defined attack signatures. Using a graph sketching technique, it summarizes long-running system execution with space efficiency to combat slow-acting attacks that take place over a long time span. UNICORN further improves its detection capability using a novel modeling approach to understand long-term behavior as the system evolves. Our evaluation shows that UNICORN outperforms an existing state-of-the-art APT detection system and detects real- life APT scenarios with high accuracy.","publisher":"NDSS 2020","url":"https://arxiv.org/abs/2001.01525","path":"/Users/wubolun/Bowen/SJTU/Netsec&TS Lab/papers/apt/UNICORN Runtime Provenance-Based Detector for Advanced Persistent Threats.pdf","tags":["APT-Detection","APT"],"remark":"基于异常的检测","_id":"iRwZTALCZZMbvFQu"}
{"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","author":null,"abstract":"We   introduce   a   new   language   representa- tion  model  calledBERT,  which  stands  for BidirectionalEncoderRepresentations  from Transformers.  Unlike recent language repre- sentation  models  (Peters  et  al.,  2018a;  Rad- ford  et  al.,  2018),  BERT  is  designed  to  pre- train  deep  bidirectional  representations  from unlabeled text by jointly conditioning on both left  and  right  context  in  all  layers.   As  a  re- sult, the pre-trained BERT model can be fine- tuned  with  just  one  additional  output  layer to  create  state-of-the-art  models  for  a  wide range of tasks, such as question answering and language  inference,  without  substantial  task- specific architecture modifications. BERT is conceptually simple and empirically powerful.   It  obtains  new  state-of-the-art  re- sults  on  eleven  natural  language  processing tasks,  including  pushing  the  GLUE  score  to 80.5%  (7.7%  point  absolute  improvement), MultiNLI  accuracy  to  86.7%  (4.6%  absolute improvement), SQuAD v1.1 question answer- ing  Test  F1  to  93.2  (1.5  point  absolute  im- provement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).","publisher":null,"url":null,"path":"/Users/wubolun/Bowen/SJTU/Netsec&TS Lab/papers/nlp/BERT.pdf","tags":["AI","MCBG"],"remark":null,"_id":"lTlilFFqI1xioCUX"}
